YourselfLM: Frontend Functional Requirements Specification

1.0 Introduction and Core Vision

This document outlines the functional and non-functional requirements for the web-based frontend of the YourselfLM platform. This frontend serves as the primary interface for the user's journey of self-discovery, powered by a sophisticated AI backend designed to analyze and reflect their personality. The user experience must be seamless, intuitive, and actively build trust between the user and the system. These requirements will guide the development of a responsive and insightful user interface that functions as a mirror to the user's mind, facilitating a deeper understanding of oneself through structured, AI-driven interaction.

2.0 System Architecture & Frontend Role

2.1 High-Level System Flow

To provide context for the frontend's role, it is essential to understand the high-level system architecture. The platform operates on a multi-stage process where the frontend is the critical starting and ending point of every user interaction. The end-to-end flow is as follows:

1. The user provides text input through the Frontend UI.
2. This input is sent to a FastAPI backend.
3. A lightweight AI model performs an initial analysis to select relevant information tags.
4. Based on these tags, the backend retrieves structured PersonData (long-term traits) and EpisodeData (specific events) from a database.
5. A heavyweight AI model (e.g., Gemma 12B) receives the user's input along with the retrieved data to generate a context-aware and persona-consistent response.
6. The response is streamed back to the Frontend UI for display to the user.

The frontend's primary responsibility is to facilitate this data flow smoothly while capturing nuanced user interactions that contribute to the system's analytical depth.

2.2 Technology Stack

The frontend will be built to interact with the following key technologies, which form the foundation of the YourselfLM ecosystem:

* Backend: FastAPI (Python)
* AI Models: Local Large Language Models (LLMs) accessed via an API, with established integration for llama.cpp.
* Frontend Technologies: The current implementation is based on JavaScript and HTML. Planning documents note the intention to use a modern framework such as React.
* API Communication: The frontend must communicate with the backend via REST API endpoints and is required to support Server-Sent Events (SSE) for real-time response streaming.

The following feature requirements are designed to be implemented upon this architectural foundation.

3.0 Core Feature Requirements

This section details the heart of the user experience—the essential components and functionalities the user will directly interact with. These features range from the primary conversational interface to mechanisms for providing feedback on the AI's analysis, forming a complete loop of interaction and refinement.

3.1 Main Conversational Interface

The chat interface is the central hub of user activity. Its components must support a responsive, intuitive, and data-rich dialogue between the user and the AI.

* Message Input Field: A standard text input area will be provided for user prompts. Crucially, this component must be instrumented to capture user behavior metrics, including typing speed, pauses, and deletion/correction events. This behavioral data is critical for the backend to analyze the user's hesitation and conviction, a metric referred to in planning documents as 迷い度 (degree of hesitation).
* Message Display Area: A chronological chat log will display user prompts and AI responses, providing a clear and standard conversational history.
* Streaming Response Handling: The interface is required to render AI responses in real-time as they are generated by the backend. This will be achieved using Server-Sent Events (SSE). This approach is critical for building user trust, as it creates a more dynamic and natural conversational feel, mimicking human interaction rather than a transactional machine query.
* Dynamic Interactive Elements: The frontend must be capable of dynamically generating and displaying HTML buttons as response options for the user. These buttons will be created based on instructions received from the backend, allowing for guided and structured conversational flows.

3.2 User Feedback Mechanism

A user-driven feedback system is a critical component for refining the AI's understanding of the user. This feature, noted as "not yet started," will follow a "pull-type" workflow where the user initiates corrections. This mechanism is fundamental to building user trust, as it provides the user with ultimate agency over their own digital reflection, ensuring the AI's understanding remains accurate and aligned.

1. UI elements, such as a button labeled "This doesn't feel right" or a similar phrase, will be associated with AI-generated analyses or summaries.
2. When the user activates this element, the interface will present a simple form or input area for them to provide corrective feedback.
3. This feedback will be sent to a dedicated backend API endpoint. The backend will use this information to update the user's PersonData, ensuring the AI's future understanding is more accurate and aligned with the user's self-perception.

3.3 Data Visualization

The frontend must include capabilities for visualizing user data and analytical results generated by the backend.

* As noted in project documents regarding "Web視覚化 (Web visualization)," the system will generate visuals using SVG (Scalable Vector Graphics). The frontend must be able to render these SVG-based charts and diagrams within the user interface.
* A sub-requirement is a function that allows the user to download these generated SVGs. This capability must implement the logic demonstrated in the reference downloadSvg JavaScript function, allowing the user to download the generated SVG with a user-specified filename.

These core features form the foundation of the user experience. However, the platform is designed with future expansion in mind to deepen the analytical process.

4.0 Future & Advanced Feature Considerations

This section outlines planned features that will enhance the depth and breadth of the self-analysis process. These capabilities are slated for future development phases and aim to provide the AI with a more holistic view of the user.

4.1 Multimodal Input Analysis

A significant future expansion involves moving beyond text-only input to incorporate multimodal data streams.

* The system will require eventual integration with the user's webcam and microphone.
* This will enable the capture and analysis of non-verbal data points, including:
  * Facial expressions
  * Voice tone
  * Posture
* The goal of this feature is to provide the AI with a richer, more comprehensive understanding of the user's emotional and psychological state, capturing nuances that are not present in text alone.

These requirements provide a comprehensive blueprint for developing the YourselfLM frontend. By adhering to this specification, the development team can create a powerful, intuitive, and trustworthy tool that empowers users on their journey toward personal insight.
